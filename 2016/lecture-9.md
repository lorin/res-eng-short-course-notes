# Lecture 9

We'll look at D4 is a specific issue...

In the study we did in '87, the expert system tech at the time: as they improved it to handle the extra D3 and D4, what happened, they stopped handling D2.

1:52:12
That was an illustration that the competence envelope doesn't just expand
indefinitely. Every immprovement... technically erroneous assumption is: as I improve the base
competence envelope the surprise outside becomes more and more residual:
dragons occur less frequently, they're smaller and less dangerous. That's a
common assumption that everybody brings(?) when they say I'm optimized.

They're saying: when I optimize, the dragons of surprise are always reduced,
the competence gets bigger and bigger, handles more and more of the variation
in the world. The biological sphere gives lie to that assumption. It is wrong.
It is not a little wrong, it is completely wrong, technically wrong. As you
get more specialized for some things, that case with expert systems, AI in the
mid-eighties, is true in general.

As you get more specialized for some things
you *will* get less capable to handle other things.

That was our example with
Darwin's finches: as they get more specialized to handle one kind of food resource
or competition for food resource, they will need to adapt, when there's a change in distribution
food resources is reduced(???).

 That's what Doyle is talking about robust yet fragile.
That as I get better, I get more optimal with respect to a subset of criteria, even when some of those criteria
include things I *don't* want to have happen, I'm guarding against certain things, I'm adding robustness in to my
optimization, not(?) trying to optimize on certain good criteria, I'm also defending against certain bad things. and I'm doing that well, I must 
get more brittle or fragile with respect to things that fall outside that ???. There are always dragons, they will always show up, they will always at some rate/tempo/size/danger,

Dragons of surprise are normal. And they're normal because it's model surprise. Yes, we misreprsent the frequency of surprise, we often very much underestimate that. But that is
not sufficient to understand the adaptive universe. Surprise is inevitable because it's about the limits of models. Your model is good and necessary because it builds a competence
envelope, and that confidence envelope should move around. But it doesn't infinitely expand to take up more and more of the space and variation in the world. 

It gets better at some things, but there are all these things that fall outside it, ??? specialize with respect to some subset, its ability to handle the other subset goes down.
There's a fundamental tradeoff between increasing your competence envelope and your ability to handle surprise ???. Those are interdependent. That's where net adaptive value....

The trick is in aggregating... I have two meaures. The problem is two measures interact. So, as I pursue optimality relative to what I think I've got(?) model, I guarantee I undermine my ability to demonstrate graceful extensibility unless I explicitly invest in building up that capability. 

The resilience engineer's responsibility is to target that resource
capability that expands grace(?) adaptive capacity. Not ??? the infinite
resources. You've got to target those resources ???? effective for graceful
extensibility.

How do we do that? Been giving you some of the hints of what we learned. And it's a process of inquiry ???

1:56:05

pressure, conflicts, adaptation → build explanations, interventions

anomaly response → diagnostic, therapeutic

want to align:
* locally adaptive
* globally adaptive

redefine constraint space


adaptive cycle
* pressure
* adapts → workaround
* florescence

can you handle
* cascades
* changing tempos

resources:
* deploy
* mobilize
* generate

synchronize across network

get better at dealing with the event you didn't plan for

There are two learning loops:

a. tuning competence envelope (continuous improvement)
b. resileince engineering